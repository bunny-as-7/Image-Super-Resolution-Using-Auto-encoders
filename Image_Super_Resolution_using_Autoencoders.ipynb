{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Super_Resolution_using_Autoencoders.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gJAxdtNX1Yyi",
        "oGLw-cCn1Y0q",
        "ZXsavBJj1Y1N",
        "qMnwvfUb1Y1O",
        "wh9OSw-D1Y1Q",
        "telVTJOQ1Y1n",
        "ksg71gnK1Y13",
        "1QGkU3q21Y2E",
        "aWdvqEyz1Y2L",
        "jEYu9t021Y2Q",
        "8mBRutcQ1Y2R",
        "Ey0Cnp2A1Y2T",
        "J0r-drht1Y3V",
        "JIMWxWjV1Y3Z",
        "1NSeNnml1Y3Z"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKA2mwNm1YyI",
        "colab_type": "text"
      },
      "source": [
        "# Image Super Resolution Using Auto-encoders \n",
        "---\n",
        "Here, we will train an Auto-encoder to convert low-quality images into high-quality using TensorFlow and Keras. \n",
        "\n",
        "\n",
        "### Auto-encoder\n",
        "An Auto-encoder is a type of Neural Network that tries to learn a representation of its input data, but in a space with much smaller dimensionality. This smaller representation is able to learn important features of the input data that can be used to later reconstruct the data. \n",
        "An auto encoder is principally composed of 3 elements: an **encoder**, a **decoder** and a **loss function**.\n",
        "* Both the encoder and decoder are usually Convolutional Neural Networks.\n",
        "* The encoder tries to reduce the dimensionality of the input while the decoder tries to recover our image from this new space. \n",
        "    * First, the **encoder** takes an input and passes it through its layers, gradually reducing the receptive field of the input. At the end of the encoder, the input is reduced to a *linear feature representation*.  \n",
        "    * This linear feature representation is then fed to the **decoder** which tries to recover the image through *upsampling* it (increasing its receptive field) gradually until it reaches the end where the output has the same dimensions as the original input. \n",
        "* This architecture is ideal for preserving the dimensionality. However, the linear compression of the input is a *lossy* process, meaning it losses information in the process.\n",
        "* The **loss function** is a way of describing a meaningful difference (or distance) between the input and output. During training, our goal is to minimize such difference so that the network will eventually lean to reconstruct the best possible output.\n",
        "\n",
        "In order to teach an auto-encoder how to reconstruct an image, we need to show it pairs of low quality and high quality images. This way, then network will try to find the patterns and important encoded visual features needed to be able to reconstruct it from the low quality version.  \n",
        "\n",
        "During training, the hidden layers will capture a **dense** (compressed) representation of the input data.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Vz_C6I1YyL",
        "colab_type": "text"
      },
      "source": [
        "### Super-Resolution\n",
        "\n",
        "Auto-encoders have many applications in image processing, especially in the Image Transformation task. Some of these applications include: \n",
        "- Denoising\n",
        "- Super-Resolution\n",
        "- Colorization\n",
        "\n",
        "Here, we'll go over the problem of **super-resolution**, where the task is to generate a high-resolution output image from a low-resolution input.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzhGkq2q1Yyj",
        "colab_type": "text"
      },
      "source": [
        "## The encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3K1DewX1Yyl",
        "colab_type": "text"
      },
      "source": [
        "Here we'll be designing the following encoder using TensorFlow and Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VstTPiLM1Yyo",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/bunny-as7/Image-Super-Resolution-Using-Auto-encoders/blob/master/model_encoder.png?raw=1\"  />\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRaxJR9U1YzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "\n",
        "# Encoder\n",
        "\n",
        "n = 256\n",
        "chan = 3\n",
        "input_img = Input(shape=(n, n, chan))\n",
        "\n",
        "l1 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(input_img)\n",
        "l2 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l1)\n",
        "l3 = MaxPooling2D(padding='same')(l2)\n",
        "l3 = Dropout(0.3)(l3)\n",
        "l4 = Conv2D(128, (3, 3),  padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l3)\n",
        "l5 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l4)\n",
        "l6 = MaxPooling2D(padding='same')(l5)\n",
        "l3 = Dropout(0.5)(l3)\n",
        "l7 = Conv2D(256, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l6)\n",
        "encoder = Model(input_img, l7)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f7kOjIW1YzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "49428eec-95f8-47d7-e04f-5c80b050535a"
      },
      "source": [
        "encoder.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 256, 256, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 128, 128, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 128, 128, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 64, 64, 256)       295168    \n",
            "=================================================================\n",
            "Total params: 555,328\n",
            "Trainable params: 555,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogGgm6_RWBh2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**Layers `l4 and l5`**: \n",
        "```python\n",
        "l4 = Conv2D(128, (3, 3),  padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l3)\n",
        "l5 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l4)\n",
        "```\n",
        "- In Layer 3, in our new reduced space of 128x128, let's put more convolutions (128 of 3x3) so that we can learn more features. Because the space is smaller, we have less information. So by having more different convolutions we can try to compensate for the loss of information. \n",
        "- This gives the network much more perspective. We can imagine these convolution filters as being like a different point of view, each at the same thing (the image). An analogy is having many people (convolution filters) on a team working together and seeing same problem from different angles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwgkECqF1Yzc",
        "colab_type": "text"
      },
      "source": [
        "## The decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St60F-5I1Yzf",
        "colab_type": "text"
      },
      "source": [
        "Let's now build our decoder!\n",
        "\n",
        "The steps are pretty much the same as in the encoder but in **reverse order**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQLpclKD1Yzh",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/bunny-as7/Image-Super-Resolution-Using-Auto-encoders/blob/master/model_decoder.png?raw=1\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwSrWJWZXI0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder\n",
        "\n",
        "l8 = UpSampling2D()(l7)\n",
        "\n",
        "l9 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l8)\n",
        "l10 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l9)\n",
        "\n",
        "l11 = add([l5, l10])\n",
        "l12 = UpSampling2D()(l11)\n",
        "l3 = Dropout(0.3)(l3)\n",
        "l13 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l12)\n",
        "l14 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l13)\n",
        "\n",
        "l15 = add([l14, l2])\n",
        "\n",
        "# chan = 3, for RGB\n",
        "decoded = Conv2D(chan, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l15)\n",
        "\n",
        "# Create our network\n",
        "autoencoder = Model(input_img, decoded)\n",
        "# You'll understand later what this is\n",
        "autoencoder_hfenn = Model(input_img, decoded)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaeojFbI1Yzs",
        "colab_type": "text"
      },
      "source": [
        "Some comments:\n",
        "\n",
        "**Layers `l11` and  `l15`** :\n",
        "```python\n",
        "l11 = add([l5, l10])\n",
        "l15 = add([l14, l2])\n",
        "```\n",
        "- Using a **merge layer** we are performing an \"add\" operation with the layer below and a layer from the encoder: \n",
        "  - `conv2D_l4 + conv2d_l7`\n",
        "  - `conv2D_2 + conv2D_9`\n",
        "- We do this for various reasons:\n",
        "  * we want to share knowledge from the encoder to the decoder like \"Hey, I'm trying to reconstruct this part of the image, did it roughly look like that to you also?\".\n",
        "  * it helps with the **\"vanishing gradient problem\"**, in which the network looses information from going deeper. Neurons at the beginning have difficulty learning in deep networks because they are too far from the deeper networks and can't have their knowledge shared. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzv1ncEo1Yz8",
        "colab_type": "text"
      },
      "source": [
        "## Complete network\n",
        "*Here*'s the final network :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE9UQBU-1Yz_",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/bunny-as7/Image-Super-Resolution-Using-Auto-encoders/blob/master/model.png?raw=1\"   />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1rpXscY1Y0J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "0d4cfd87-1bd7-41c8-8d00-a80baeda675a"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 256, 256, 64) 1792        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 128, 128, 64) 0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 128, 128, 128 73856       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 128, 128, 128 147584      conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 64, 64, 256)  295168      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 256 0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 128, 128, 128 295040      up_sampling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 128, 128, 128 147584      conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 128, 128, 128 0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 128 0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 256, 256, 64) 73792       up_sampling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 256, 256, 64) 0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 256, 256, 3)  1731        add_3[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,110,403\n",
            "Trainable params: 1,110,403\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGJbiJ8v1Y0O",
        "colab_type": "text"
      },
      "source": [
        "### Training \n",
        "\n",
        "Let's *compile* the model to be able to train it. For now we'll simply use a **Mean Squared Error (MSE)** for the loss, we'll see later what this is and if we can go further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQYqnLHX1Y0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwHfWGBL1Y0T",
        "colab_type": "text"
      },
      "source": [
        "We have a dataset of images and we load it by batches.  the dataset doesn't really hold in memory, so we split it by batches and give it to the GPU so that it can train on a reasonable part of the dataset at each iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fkzqHc5q1Y0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "from scipy import ndimage, misc\n",
        "from skimage.transform import resize, rescale\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "def train_batches(just_load_dataset=False):\n",
        "\n",
        "    batches = 256 \n",
        "\n",
        "    batch = 0 \n",
        "    batch_nb = 0 \n",
        "    max_batches = -1 \n",
        "    \n",
        "    ep = 4 \n",
        "\n",
        "    images = []\n",
        "    x_train_n = []\n",
        "    x_train_down = []\n",
        "    \n",
        "    x_train_n2 = [] \n",
        "    x_train_down2 = []\n",
        "    \n",
        "    for root, dirnames, filenames in os.walk(\"/home/rhyme/Desktop/Project/data/cars_train\"):\n",
        "        for filename in filenames:\n",
        "            if re.search(\"\\.(jpg|jpeg|JPEG|png|bmp|tiff)$\", filename):\n",
        "                if batch_nb == max_batches: \n",
        "                    return x_train_n2, x_train_down2\n",
        "                filepath = os.path.join(root, filename)\n",
        "                image = pyplot.imread(filepath)\n",
        "                if len(image.shape) > 2:\n",
        "                        \n",
        "                    image_resized = resize(image, (256, 256))\n",
        "                    x_train_n.append(image_resized)\n",
        "                    x_train_down.append(rescale(rescale(image_resized, 0.5), 2.0))\n",
        "                    batch += 1\n",
        "                    if batch == batches:\n",
        "                        batch_nb += 1\n",
        "\n",
        "                        x_train_n2 = np.array(x_train_n)\n",
        "                        x_train_down2 = np.array(x_train_down)\n",
        "                        \n",
        "                        if just_load_dataset:\n",
        "                            return x_train_n2, x_train_down2\n",
        "                        \n",
        "                        print('Training batch', batch_nb, '(', batches, ')')\n",
        "\n",
        "                        autoencoder.fit(x_train_down2, x_train_n2,\n",
        "                            epochs=ep,\n",
        "                            batch_size=10,\n",
        "                            shuffle=True,\n",
        "                            validation_split=0.15)\n",
        "                    \n",
        "                        x_train_n = []\n",
        "                        x_train_down = []\n",
        "                    \n",
        "                        batch = 0\n",
        "\n",
        "    return x_train_n2, x_train_down2\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BSxLlC51Y0Z",
        "colab_type": "text"
      },
      "source": [
        "To train the model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnn-4MW11Y0b",
        "colab_type": "text"
      },
      "source": [
        "```Python\n",
        "x_train_n = []\n",
        "x_train_down = []\n",
        "x_train_n, x_train_down = train_batches()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-PtY4Va1Y0e",
        "colab_type": "text"
      },
      "source": [
        "Training from scratch takes a lot of time, so we'll just load a pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dMdX4pyZ90L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download CARS DATA from this url: https://ai.stanford.edu/~jkrause/cars/car_dataset.html\n",
        "\n",
        "%cd /content\n",
        "!wget http://imagenet.stanford.edu/internal/car196/cars_train.tgz \n",
        "!tar -xvzf cars_train.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYvpKEm91Y0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_n, x_train_down = train_batches(just_load_dataset=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYeIlN1b1Y0l",
        "colab_type": "text"
      },
      "source": [
        "And here, we load the already existing weights :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQUbtXNi1Y0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "0b939b6d-94af-4b97-ac2b-0bd589342fa7"
      },
      "source": [
        "# download the WEIGHTS from a location in Dropbox\n",
        "%cd /content\n",
        "!wget https://www.dropbox.com/s/n2s2n29ja5xytc7/weights.zip?dl=0 -O weights.zip\n",
        "!unzip weights.zip\n",
        "  \n",
        "autoencoder.load_weights(\"/content/weights/sr.img_net.mse.final_model5.no_patch.weights.best.hdf5\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "--2020-08-04 17:10:51--  https://www.dropbox.com/s/n2s2n29ja5xytc7/weights.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:6018:1::a27d:301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/n2s2n29ja5xytc7/weights.zip [following]\n",
            "--2020-08-04 17:10:51--  https://www.dropbox.com/s/raw/n2s2n29ja5xytc7/weights.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc3b58467965f88fabdadee9a0fe.dl.dropboxusercontent.com/cd/0/inline/A82VJ2RqyGdexkxSwX7IgklHemIaiCQXUVEXbZGBtw3efTtpNbHxnCC7Hudx6jzgH0pHe_qsjtq3mugJNcRwFCTsqdm56_oKyC0lsvmxD-T1Aw/file# [following]\n",
            "--2020-08-04 17:10:53--  https://uc3b58467965f88fabdadee9a0fe.dl.dropboxusercontent.com/cd/0/inline/A82VJ2RqyGdexkxSwX7IgklHemIaiCQXUVEXbZGBtw3efTtpNbHxnCC7Hudx6jzgH0pHe_qsjtq3mugJNcRwFCTsqdm56_oKyC0lsvmxD-T1Aw/file\n",
            "Resolving uc3b58467965f88fabdadee9a0fe.dl.dropboxusercontent.com (uc3b58467965f88fabdadee9a0fe.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc3b58467965f88fabdadee9a0fe.dl.dropboxusercontent.com (uc3b58467965f88fabdadee9a0fe.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/A81w7yNp6LAzGQ8CoqmGYL9MUOPjJznrYm57i4dKPmpgofPcIMB6J9Gh73oTqaBQSRPnaiNoookbYiHBYOUu2puIJMXwFQdRX4aSkYpSiVuOqpzs5wB1RKbggMznRgs6ASgJSMlo3KLFL6gc8hi9Mib1KIQinUoIbNw_n4fIi_M8vLt3gDZ_d_zfUHfpTKGcLZcgGcUkHajl6_3w8g3jkPSYgzP1cGqXGjdZVX4MUPPIwhHFLXqWkusqw8R10PCPaa2U5CKDd71-bkOWnMqnUwzwk9p1mWF6H4E7kkdjoItRBnQah6D8rmzZYBFKWHyBQ_QXNSVhpEepf7_OmGilDZOQ/file [following]\n",
            "--2020-08-04 17:10:54--  https://uc3b58467965f88fabdadee9a0fe.dl.dropboxusercontent.com/cd/0/inline2/A81w7yNp6LAzGQ8CoqmGYL9MUOPjJznrYm57i4dKPmpgofPcIMB6J9Gh73oTqaBQSRPnaiNoookbYiHBYOUu2puIJMXwFQdRX4aSkYpSiVuOqpzs5wB1RKbggMznRgs6ASgJSMlo3KLFL6gc8hi9Mib1KIQinUoIbNw_n4fIi_M8vLt3gDZ_d_zfUHfpTKGcLZcgGcUkHajl6_3w8g3jkPSYgzP1cGqXGjdZVX4MUPPIwhHFLXqWkusqw8R10PCPaa2U5CKDd71-bkOWnMqnUwzwk9p1mWF6H4E7kkdjoItRBnQah6D8rmzZYBFKWHyBQ_QXNSVhpEepf7_OmGilDZOQ/file\n",
            "Reusing existing connection to uc3b58467965f88fabdadee9a0fe.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37939910 (36M) [application/zip]\n",
            "Saving to: ‘weights.zip’\n",
            "\n",
            "weights.zip         100%[===================>]  36.18M  40.3MB/s    in 0.9s    \n",
            "\n",
            "2020-08-04 17:10:56 (40.3 MB/s) - ‘weights.zip’ saved [37939910/37939910]\n",
            "\n",
            "Archive:  weights.zip\n",
            "replace weights/sr.img_net.mse.final_model5.no_patch.weights.best.hdf5? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: weights/sr.img_net.mse.final_model5.no_patch.weights.best.hdf5  \n",
            "replace __MACOSX/weights/._sr.img_net.mse.final_model5.no_patch.weights.best.hdf5? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/weights/._sr.img_net.mse.final_model5.no_patch.weights.best.hdf5  \n",
            "replace weights/sr.img_net.mse.final_model5.patch.weights.best.hdf5? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: weights/sr.img_net.mse.final_model5.patch.weights.best.hdf5  \n",
            "replace __MACOSX/weights/._sr.img_net.mse.final_model5.patch.weights.best.hdf5? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: __MACOSX/weights/._sr.img_net.mse.final_model5.patch.weights.best.hdf5  \n",
            "  inflating: weights/encoder_weights.hdf5  \n",
            "  inflating: __MACOSX/weights/._encoder_weights.hdf5  \n",
            "  inflating: weights/sr.img_net.mse_hfenn.final_model5_2.no_patch.weights.best.hdf5  \n",
            "  inflating: __MACOSX/weights/._sr.img_net.mse_hfenn.final_model5_2.no_patch.weights.best.hdf5  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGLw-cCn1Y0q",
        "colab_type": "text"
      },
      "source": [
        "### Display the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go6LJrVX1Y03",
        "colab_type": "text"
      },
      "source": [
        "Predict:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--HPXY-j1Y04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We clip the output so that it doesn't produce weird colors\n",
        "#sr1 = np.clip(autoencoder.predict(x_train_down), 0.0, 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZapUSRU1Y09",
        "colab_type": "text"
      },
      "source": [
        "Display and compare the results. In the following order:\n",
        "\n",
        "* The low-res input image\n",
        "* The reconstructed image\n",
        "* The original high-res image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcQd3TM1Y0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_index = 251"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsn1b7jV1Y1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(128, 128))\n",
        "i = 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index])  #Input (low-res)\n",
        "i += 1\n",
        "#ax = plt.subplot(10, 10, i)\n",
        "#plt.imshow(sr1[image_index])  # Output (supre-res recovered image)\n",
        "#i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_n[image_index])  # Ground truth (high-res) \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2SD2si31Y1d",
        "colab_type": "text"
      },
      "source": [
        "## Loss Metrics\n",
        "\n",
        "Below are the most common metrics to measure the similarity between a pair of images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "telVTJOQ1Y1n",
        "colab_type": "text"
      },
      "source": [
        "### MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3viqcExU1Y1q",
        "colab_type": "text"
      },
      "source": [
        "[Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is a metric that indicates perfect similarity if the value is 0.\n",
        "\n",
        "The value grows beyond 0 if the average difference between pixel intensities increases.\n",
        "\n",
        "MSE has a few issues when used for similarity comparison, meaning that if you take an image and photoshop it to make it brighter and compare it with the original, the two images will be very different according to MSE as a dark image has pixel values closer to 0 and a bright image has pixel closer to 1, this means that the difference is going to be very big as MSE sees the image from a very general point of view.\n",
        "\n",
        "Anyway, for our purpose, we compare lower quality images to their higher resolution counterpart, so the brightness is retained, this means that ** MSE is still a useful metric in our case**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5401v-L1Y1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse(orig, res):\n",
        "    return ((orig - res) ** 2).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wda6KVFW1Y1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mse(high_res, low_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksg71gnK1Y13",
        "colab_type": "text"
      },
      "source": [
        "### SSIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko2ybLmQ1Y14",
        "colab_type": "text"
      },
      "source": [
        "[Structural similarity (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity) measures the similarity between two images as would be perceived on a television or a similar media."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBQ81FP71Y15",
        "colab_type": "text"
      },
      "source": [
        "The SSIM is a value in the range $[1, -1]$ where $1$ would mean two indentical images, and lower values would show a \"perceptual\" difference.\n",
        "\n",
        "This metric compares small windows in the image rather than the whole image (like MSE), which makes it a bit more interesting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7goFz-91Y17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import skimage.measure\n",
        "import os\n",
        "from matplotlib import pyplot\n",
        "\n",
        "def ssim(ori, res):\n",
        "    return skimage.measure.compare_ssim(ori.astype(np.float64),\n",
        "        res.astype(np.float64),\n",
        "        gaussian_weights=True, data_range=1., win_size=1,\n",
        "        sigma=1.5, multichannel=False, use_sample_covariance=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7FsuS9A1Y2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ssim(high_res, low_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QGkU3q21Y2E",
        "colab_type": "text"
      },
      "source": [
        "### PSNR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdEHg-OA1Y2E",
        "colab_type": "text"
      },
      "source": [
        "[Peak signal-to-noise ratio](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio) is a metric defined using Mean Squared Error (seen above). \n",
        "\n",
        "*PSNR is most commonly used to measure the quality of reconstruction of lossy compression* - Wikipedia\n",
        "\n",
        "Low resolution and pixelization can be considered as a form of *compression* as we loose information.\n",
        "\n",
        "When there's no noise, the PSNR is infinite (because there's a division by the MSE and MSE is $0$ when both images are exactly the same).\n",
        "\n",
        "Of course, this means that we need to maximize the PSNR. The result is in decibel (dB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD7yncvH1Y2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def psnr(ori, res):\n",
        "    return skimage.measure.compare_psnr(ori, res, data_range=1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l7-xAxZ1Y2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "psnr(high_res, low_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWdvqEyz1Y2L",
        "colab_type": "text"
      },
      "source": [
        "### HFENN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MwI6Zf51Y2M",
        "colab_type": "text"
      },
      "source": [
        "The HFENN (High Frequency Error Norm Normalized) metric gives  a  measure  of how high-frequency details differ between two images.\n",
        "\n",
        "This means that we can guess if an image has more or less high frequency details (which are fine details that you need to zoom in to see and that are not blurry) compared to another image.\n",
        "\n",
        "When the output value is 0 the images are identical. The greater the value, the more of a perceptual difference in both images there is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FksgXrwa1Y2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import filters\n",
        "\n",
        "def l_o_g(img, sigma):\n",
        "    '''\n",
        "    Laplacian of Gaussian filter (channel-wise)\n",
        "    -> img: input image\n",
        "    -> sigma: gaussian_laplace sigma\n",
        "    <- filtered image\n",
        "    '''\n",
        "    while len(img.shape) < 3:\n",
        "        img = img[..., np.newaxis]\n",
        "    out = img.copy()\n",
        "    for chan in range(img.shape[2]):\n",
        "        out[..., chan] = filters.gaussian_laplace(img[..., chan], sigma)\n",
        "    return out\n",
        "\n",
        "def hfenn(orig, res):\n",
        "    '''\n",
        "    High Frequency Error Norm (Normalized) metric for comparison of original and result images\n",
        "    The metric independent to image size (in contrast to regular HFEN)\n",
        "    Inputs are expected to be float in range [0, 1] (with possible overflow)\n",
        "    -> ori: original image\n",
        "    -> res: result image\n",
        "    <- HFENN value\n",
        "    '''\n",
        "    sgima = 1.5  # From DLMRI paper\n",
        "    return np.mean((l_o_g(orig - res, sgima)) ** 2) * 1e4  # magnification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BNf9e7Q1Y2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hfenn(high_res, low_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEYu9t021Y2Q",
        "colab_type": "text"
      },
      "source": [
        "## Combining Loss Functions\n",
        "\n",
        "Let's try using a loss that doesn't just tell us the pixel-wise difference in resolution, but that also if there's an improvement, for example, in high frequency details.\n",
        "\n",
        "### MSE and HFENN\n",
        "\n",
        "We can do a weight sum of both losses like:\n",
        "\n",
        "$MSE + weight * HFENN$\n",
        "\n",
        "We could choose $weight = 10$ and see what happens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4qFCAo61Y2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.ndimage as nd\n",
        "import scipy.ndimage.filters as filters\n",
        "from keras import losses\n",
        "import tensorflow as tf\n",
        "\n",
        "def hfenn_loss(ori, res):\n",
        "    '''\n",
        "    HFENN-based loss\n",
        "    ori, res - batched images with 3 channels\n",
        "    See metrics.hfenn\n",
        "    '''\n",
        "    fnorm = 0.325 # norm of l_o_g operator, estimated numerically\n",
        "    sigma = 1.5 # parameter from HFEN metric\n",
        "    truncate = 4 # default parameter from filters.gaussian_laplace\n",
        "    wradius = int(truncate * sigma + 0.5)\n",
        "    eye = np.zeros((2*wradius+1, 2*wradius+1), dtype=np.float32)\n",
        "    eye[wradius, wradius] = 1.\n",
        "    ker_mat = filters.gaussian_laplace(eye, sigma)\n",
        "    with tf.name_scope('hfenn_loss'):\n",
        "        chan = 3\n",
        "        ker = tf.constant(np.tile(ker_mat[:, :, None, None], (1, 1, chan, 1)))\n",
        "        filtered = tf.nn.depthwise_conv2d(ori - res, ker, [1, 1, 1, 1], 'VALID')\n",
        "        loss = tf.reduce_mean(tf.square(filtered))\n",
        "        loss = loss / (fnorm**2)\n",
        "    return loss\n",
        "  \n",
        "\n",
        "def ae_loss(input_img, decoder):\n",
        "    mse = losses.mean_squared_error(input_img, decoder) # MSE\n",
        "    weight = 10.0 # weight\n",
        "    return mse + weight * hfenn_loss(input_img, decoder) # MSE + weight * HFENN\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moSB0zLW1Y2g",
        "colab_type": "text"
      },
      "source": [
        "Now, just compile the model with the new loss :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp6XzvYw1Y2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.compile(optimizer='adadelta', loss=ae_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr6c6AE71Y2l",
        "colab_type": "text"
      },
      "source": [
        "If you wanted to train the model, that's how you'd do :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q_MrEDV1Y2l",
        "colab_type": "text"
      },
      "source": [
        "```Python\n",
        "x_train_n = []\n",
        "x_train_down = []\n",
        "x_train_n, x_train_down = train_batches()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "121A8cWL1Y2m",
        "colab_type": "text"
      },
      "source": [
        "But, we'll just load the pretrained weights :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApyDoi-O1Y2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO\n",
        "## download the weights\n",
        "\n",
        "autoencoder_hfenn.load_weights(\"/data/sr.img_net.mse_hfenn.final_model5_2.no_patch.weights.best.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XqH_53V1Y2t",
        "colab_type": "text"
      },
      "source": [
        "Let's see what the network can do after using our new custom loss :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9NuD7zV1Y2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sr_hfenn = np.clip(autoencoder_hfenn.predict(x_train_down), 0.0, 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKHBNKzP1Y21",
        "colab_type": "text"
      },
      "source": [
        "Display the image results:\n",
        "\n",
        "* The low resolution input image\n",
        "* A bicubic interopolated version\n",
        "* The reconstructed image with MSE\n",
        "* The reconstructed image with our custom MSE + HFENN loss\n",
        "* The original perfect image\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jETO4hJs1Y26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_index = 99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hSKatI71Y29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(128, 128))\n",
        "i = 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index], interpolation=\"bicubic\")\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(sr1[image_index])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(sr_hfenn[image_index])  # The reconstructed image with our custom MSE + HFENN loss\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_n[image_index])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PDB8wuL1Y3W",
        "colab_type": "text"
      },
      "source": [
        "If you look a bit closely and check out the lines and edges, you'll se that they're sharper when using MSE and HFENN compared to MSE alone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE28_Swy1Y3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(128, 128))\n",
        "j = 6\n",
        "i = 1\n",
        "idx_1 = 32*j\n",
        "idx_2 = 32*(j+1)\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index, idx_1:idx_2, idx_1:idx_2])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index, idx_1:idx_2, idx_1:idx_2], interpolation=\"bicubic\")\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(sr1[image_index, idx_1:idx_2, idx_1:idx_2])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(sr_hfenn[image_index, idx_1:idx_2, idx_1:idx_2])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_n[image_index, idx_1:idx_2, idx_1:idx_2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ5F8U6s1Y3Y",
        "colab_type": "text"
      },
      "source": [
        "**Note** : double click on the images to make them bigger"
      ]
    }
  ]
}